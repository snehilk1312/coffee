{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and install any necessary packages\n",
    "import spacy\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the directory containing the attribute_cleaner module\n",
    "sys.path.append(os.path.abspath(\"../transformer\"))\n",
    "\n",
    "# Now you can import text_cleaner\n",
    "from attribute_cleaner.general_string_cleaner import text_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    }
   ],
   "source": [
    "# reading the data in doccano jsonl output format\n",
    "with open('data/merged_data.jsonl', 'r') as f:\n",
    "    lines = list(f)\n",
    "\n",
    "training_data: list = []\n",
    "\n",
    "for line in lines:\n",
    "    row = json.loads(line)\n",
    "    if row['label']:\n",
    "        training_data.append(  [ row[\"text\"], { \"entities\": row[\"label\"] } ] )\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"savorworks x mooleh manay ! a collaboration that has been in the works for 4 years and it \\' s shaping up to be something truly \"\" phenomenal \"\" . this year \\' s phenom promises an extraordinary experience as we proudly announce our partnership with mooley manay estate , situated near the beautiful backwaters of the harangi reservoir in coorg , karnataka . producers komal and akshay dashrath have masterfully embraced a data - driven approach to farming and coffee processing . their unwavering belief in the pivotal role of data in optimizing agricultural techniques and ensuring the utmost quality in coffee production is truly inspiring . this coffee is a 64 hours yeast inoculated - natural . the process begins with hand - picking ripe coffee cherries and floating them in water . ripe , denser cherries sink , while unripe or damaged ones float and are removed . the coffee is then placed in sealed tanks for a controlled , anaerobic fermentation lasting 64 hours , during which a custom yeast culture is introduced to the coffee . after fermentation , the coffee is transferred to a polyhouse and spread on raised drying beds . it is regularly raked to ensure even drying , which continues until the coffee reaches a specific moisture level ( around 12 - 11% ) . once dry , the coffee is bagged and stored for 2 months before further processing . facts : origin : mooleh manay estate altitude : 1000 masl varietal : sln - 6 process : 64 hours yeast inoculated - natural roast level : medium roast rite color ( whole bean ) : 64 roast rite color ( ground ) : 75 feels : tasting notes : pink guava , orange marmalade , plum and banana . acidity : juicy body : medium aftertaste : very long\"',\n",
       " {'entities': [[1, 11, 'ROASTER'],\n",
       "   [14, 26, 'ESTATE'],\n",
       "   [161, 167, 'NAME'],\n",
       "   [249, 261, 'ESTATE'],\n",
       "   [338, 343, 'LOCATION'],\n",
       "   [346, 355, 'LOCATION'],\n",
       "   [368, 373, 'FARMER'],\n",
       "   [378, 384, 'FARMER'],\n",
       "   [385, 393, 'FARMER'],\n",
       "   [661, 669, 'FERMENTATION TIME'],\n",
       "   [963, 971, 'FERMENTATION TIME'],\n",
       "   [1373, 1385, 'ESTATE'],\n",
       "   [1404, 1408, 'ELEVATION'],\n",
       "   [1425, 1432, 'PROCESSING'],\n",
       "   [1443, 1478, 'PROCESSING'],\n",
       "   [1493, 1499, 'ROAST LEVEL'],\n",
       "   [1594, 1604, 'TASTING NOTES'],\n",
       "   [1607, 1623, 'TASTING NOTES'],\n",
       "   [1626, 1630, 'TASTING NOTES'],\n",
       "   [1635, 1641, 'TASTING NOTES'],\n",
       "   [1654, 1659, 'ACIDITY'],\n",
       "   [1667, 1673, 'BODY'],\n",
       "   [1687, 1696, 'AFTERTASTE']]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create spaCy DocBin objects from the annotated data\n",
    "def get_spacy_doc(file, data):\n",
    "  # Create a blank spaCy pipeline\n",
    "  nlp = spacy.blank('en')\n",
    "  db = DocBin()\n",
    "\n",
    "  # Iterate through the data\n",
    "  for text, annot in tqdm(data):\n",
    "    doc = nlp.make_doc(text)\n",
    "    annot = annot['entities']\n",
    "\n",
    "    ents = []\n",
    "    entity_indices = []\n",
    "\n",
    "    # Extract entities from the annotations\n",
    "    for start, end, label in annot:\n",
    "      skip_entity = False\n",
    "      for idx in range(start, end):\n",
    "        if idx in entity_indices:\n",
    "          skip_entity = True\n",
    "          break\n",
    "      if skip_entity:\n",
    "        continue\n",
    "\n",
    "      entity_indices = entity_indices + list(range(start, end))\n",
    "      try:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "      if span is None:\n",
    "        # Log errors for annotations that couldn't be processed\n",
    "        err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
    "        file.write(err_data)\n",
    "      else:\n",
    "        ents.append(span)\n",
    "\n",
    "    try:\n",
    "      doc.ents = ents\n",
    "      db.add(doc)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 1110.77it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 697.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the annotated data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(training_data, test_size=0.1)\n",
    "\n",
    "# Display the number of items in the training and testing sets\n",
    "print(len(train), len(test))\n",
    "\n",
    "# Open a file to log errors during annotation processing\n",
    "file = open('train_file_try_5.txt','w')\n",
    "\n",
    "# Create spaCy DocBin objects for training and testing data\n",
    "db = get_spacy_doc(file, train)\n",
    "db.to_disk('train_doccano_try_5.spacy')\n",
    "\n",
    "db = get_spacy_doc(file, test)\n",
    "db.to_disk('test_doccano_try_5.spacy')\n",
    "\n",
    "# Close the error log file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config_ner_en_blank.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config_ner_en_blank.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# configure spacy for custom NER model , using a base config file - https://spacy.io/usage/training#config\n",
    "\n",
    "# !python -m spacy init fill-config config_cpu_en_blank_base.cfg config_ner_en_blank.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: output_try_5\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: output_try_5\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    102.92    0.00    0.00    0.00    0.00\n",
      "  0      50        360.85   3088.34    0.00    0.00    0.00    0.00\n",
      "  0     100       1627.55   1139.89    2.25    8.33    1.30    0.02\n",
      "  0     150         59.82    850.47   28.01   39.68   21.65    0.28\n",
      "  1     200        229.60    807.99   43.97   57.75   35.50    0.44\n",
      "  1     250         54.14    649.97   46.87   63.24   37.23    0.47\n",
      "  1     300       1984.74    721.40   44.75   61.83   35.06    0.45\n",
      "  2     350        356.52    625.71   53.40   60.77   47.62    0.53\n",
      "  2     400        144.80    483.87   50.37   58.62   44.16    0.50\n",
      "  2     450        167.30    700.62   53.09   65.61   44.59    0.53\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output_try_5/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config_ner_en_blank.cfg  --output  output_try_5  --paths.train train_doccano_try_5.spacy  --paths.dev  test_doccano_try_5.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and trying the model with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy library\n",
    "import spacy\n",
    "\n",
    "# Load the trained spaCy NER model from the specified path\n",
    "nlp = spacy.load('output_try_5/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Jasmine kissed cranberry - 100 % arabica that is sourced from Chikmagalur, grown at an altitude of approximately 4,100ft to 4,500ft. It gets its unique name through a process which include a strain of yeast used to create a carbon dioxide-rich environment during fermentation in stainless-steel fermenters. A remarkable level of complexity in the beans is created by this process, laying the groundwork for a wonderful cup of coffee. Steady drying on raised beds is the next important step, which lets the flavors gradually develop and intensify. After another 30 days of continuous stirring, the coffee undergo additional drying developing scent of jasmine, complex notes of raspberries, cranberries, and sparkling malic acidity with a lingering floral aftertaste \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jasmine   ->>>>   TASTING NOTES\n",
      "arabica   ->>>>   COFFEE TYPE\n",
      "chikmagalur   ->>>>   LOCATION\n",
      "jasmine   ->>>>   TASTING NOTES\n",
      "raspberries   ->>>>   TASTING NOTES\n",
      "cranberries   ->>>>   TASTING NOTES\n",
      "malic acidity   ->>>>   ESTATE\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text using the loaded spaCy NER model\n",
    "doc = nlp(text_cleaner(text))\n",
    "\n",
    "# Iterate through the named entities (entities) recognized by the model\n",
    "for ent in doc.ents:\n",
    "  # Print the recognized text and its corresponding label\n",
    "  print(ent.text, \"  ->>>>  \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"\n",
    "Single origin Indian Coffee From Moganad Estate, Tamil Nadu which is a 100% Arabiaca coffee with \n",
    "an exquisite blend of balanced sweetness and brightness. A Medium Dark Roast coffee with \n",
    "flavor notes of Cocoa, Caramel and Nut which can also be enjoyed in a French press, moka pot, aeropress & espresso .\n",
    " It is washed processed, grown at an altitude of around 4430 ft\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moganad   ->>>>   ESTATE\n",
      "medium dark roast   ->>>>   ROAST LEVEL\n",
      "cocoa   ->>>>   TASTING NOTES\n",
      "caramel   ->>>>   TASTING NOTES\n",
      "4430 ft   ->>>>   ELEVATION\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text using the loaded spaCy NER model\n",
    "doc = nlp(text_cleaner(text))\n",
    "\n",
    "# Iterate through the named entities (entities) recognized by the model\n",
    "for ent in doc.ents:\n",
    "  # Print the recognized text and its corresponding label\n",
    "  print(ent.text, \"  ->>>>  \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
