{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and install any necessary packages\n",
    "import spacy\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the directory containing the attribute_cleaner module\n",
    "sys.path.append(os.path.abspath(\"../transformer\"))\n",
    "\n",
    "# Now you can import text_cleaner\n",
    "from attribute_cleaner.general_string_cleaner import text_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    }
   ],
   "source": [
    "# reading the data in doccano jsonl output format\n",
    "with open('data/merged_data.jsonl', 'r') as f:\n",
    "    lines = list(f)\n",
    "\n",
    "training_data: list = []\n",
    "\n",
    "for line in lines:\n",
    "    row = json.loads(line)\n",
    "    if row['label']:\n",
    "        training_data.append(  [ row[\"text\"], { \"entities\": row[\"label\"] } ] )\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"our latest washed coffee is also the sweetest ! sourced from krishnagiri estate , high up in the chandra drona hills ( a . k . a the giris ) of chikmagalur , this lot of washed arabica fascinated us with its bean density and versatility ( traits we have come to associate with the coffee from krishnagiri estate ) . ripe cherries of the chandragiri varietal were picked for this lot and rested before pulping . the cherries were rested some more post pulping and then washed and slow-dried . the resulting cup is very sweet and delightfully nutty with a mildly cherry like finish . this sugar like sweetness reminds us of the rāga mohanakalyāni and this coffee is the new daily driver at kāpikottai hq . mohanakalyāni the coffee , is a delight across all manual brews and also shines as a single origin espresso\"',\n",
       " {'entities': [[12, 18, 'PROCESSING'],\n",
       "   [62, 73, 'ESTATE'],\n",
       "   [98, 117, 'LOCATION'],\n",
       "   [145, 156, 'LOCATION'],\n",
       "   [178, 185, 'COFFEE TYPE'],\n",
       "   [294, 305, 'ESTATE'],\n",
       "   [338, 349, 'VARIETAL'],\n",
       "   [542, 547, 'TASTING NOTES'],\n",
       "   [562, 568, 'TASTING NOTES'],\n",
       "   [689, 699, 'ROASTER'],\n",
       "   [705, 718, 'NAME']]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create spaCy DocBin objects from the annotated data\n",
    "def get_spacy_doc(file, data):\n",
    "  # Create a blank spaCy pipeline\n",
    "  nlp = spacy.blank('en')\n",
    "  db = DocBin()\n",
    "\n",
    "  # Iterate through the data\n",
    "  for text, annot in tqdm(data):\n",
    "    doc = nlp.make_doc(text)\n",
    "    annot = annot['entities']\n",
    "\n",
    "    ents = []\n",
    "    entity_indices = []\n",
    "\n",
    "    # Extract entities from the annotations\n",
    "    for start, end, label in annot:\n",
    "      skip_entity = False\n",
    "      for idx in range(start, end):\n",
    "        if idx in entity_indices:\n",
    "          skip_entity = True\n",
    "          break\n",
    "      if skip_entity:\n",
    "        continue\n",
    "\n",
    "      entity_indices = entity_indices + list(range(start, end))\n",
    "      try:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "      if span is None:\n",
    "        # Log errors for annotations that couldn't be processed\n",
    "        err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
    "        file.write(err_data)\n",
    "      else:\n",
    "        ents.append(span)\n",
    "\n",
    "    try:\n",
    "      doc.ents = ents\n",
    "      db.add(doc)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snehil/.dev_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 164/164 [00:00<00:00, 1048.46it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 642.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the annotated data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(training_data, test_size=0.1)\n",
    "\n",
    "# Display the number of items in the training and testing sets\n",
    "print(len(train), len(test))\n",
    "\n",
    "# Open a file to log errors during annotation processing\n",
    "file = open('train_file_try_4.txt','w')\n",
    "\n",
    "# Create spaCy DocBin objects for training and testing data\n",
    "db = get_spacy_doc(file, train)\n",
    "db.to_disk('train_doccano_try_4.spacy')\n",
    "\n",
    "db = get_spacy_doc(file, test)\n",
    "db.to_disk('test_doccano_try_4.spacy')\n",
    "\n",
    "# Close the error log file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure spacy for custom NER model , using a base config file - https://spacy.io/usage/training#config\n",
    "\n",
    "# !python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: output_try_3\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: output_try_3\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "/home/snehil/.dev_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/snehil/.dev_env/lib/python3.12/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "/home/snehil/.dev_env/lib/python3.12/site-packages/thinc/shims/pytorch.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "  0       0        1042.32   1636.80    0.54    0.28    5.26    0.01\n",
      "  2      50       23101.30  34825.88    0.00    0.00    0.00    0.00\n",
      "  5     100        9760.66   6768.00   48.16   66.06   37.89    0.48\n",
      "  7     150       47481.61   6347.07   44.02   45.51   42.63    0.44\n",
      " 10     200       42633.35   6066.12   52.75   55.17   50.53    0.53\n",
      " 13     250       22395.17   4309.18   55.40   58.48   52.63    0.55\n",
      " 15     300        5802.74   3511.73   59.68   60.99   58.42    0.60\n",
      " 18     350        8555.23   2814.11   61.05   53.82   70.53    0.61\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output_try_3/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg  --output  output_try_4  --paths.train train_doccano_try_4.spacy  --paths.dev  test_doccano_try_4.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and trying the model with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snehil/.dev_env/lib/python3.12/site-packages/spacy_transformers/layers/hf_shim.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Import the spaCy library\n",
    "import spacy\n",
    "\n",
    "# Load the trained spaCy NER model from the specified path\n",
    "nlp = spacy.load('output_try_3/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Jasmine kissed cranberry - 100 % arabica that is sourced from Chikmagalur, grown at an altitude of approximately 4,100ft to 4,500ft. It gets its unique name through a process which include a strain of yeast used to create a carbon dioxide-rich environment during fermentation in stainless-steel fermenters. A remarkable level of complexity in the beans is created by this process, laying the groundwork for a wonderful cup of coffee. Steady drying on raised beds is the next important step, which lets the flavors gradually develop and intensify. After another 30 days of continuous stirring, the coffee undergo additional drying developing scent of jasmine, complex notes of raspberries, cranberries, and sparkling malic acidity with a lingering floral aftertaste \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snehil/.dev_env/lib/python3.12/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jasmine   ->>>>   TASTING NOTES\n",
      "arabica   ->>>>   COFFEE TYPE\n",
      "chikmagalur   ->>>>   LOCATION\n",
      "4 , 100ft   ->>>>   ELEVATION\n",
      "jasmine   ->>>>   TASTING NOTES\n",
      "raspberries   ->>>>   TASTING NOTES\n",
      "cranberries   ->>>>   TASTING NOTES\n",
      "malic   ->>>>   ACIDITY\n",
      "floral   ->>>>   COFFEE_PROPERTIES\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text using the loaded spaCy NER model\n",
    "doc = nlp(text_cleaner(text))\n",
    "\n",
    "# Iterate through the named entities (entities) recognized by the model\n",
    "for ent in doc.ents:\n",
    "  # Print the recognized text and its corresponding label\n",
    "  print(ent.text, \"  ->>>>  \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"\n",
    "Single origin Indian Coffee From Moganad Estate, Tamil Nadu which is a 100% Arabiaca coffee with \n",
    "an exquisite blend of balanced sweetness and brightness. A Medium Dark Roast coffee with \n",
    "flavor notes of Cocoa, Caramel and Nut which can also be enjoyed in a French press, moka pot, aeropress & espresso .\n",
    " It is washed processed, grown at an altitude of around 4430 ft\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moganad   ->>>>   ESTATE\n",
      "tamil nadu   ->>>>   LOCATION\n",
      "arabiaca   ->>>>   COFFEE TYPE\n",
      "medium dark   ->>>>   ROAST LEVEL\n",
      "cocoa   ->>>>   TASTING NOTES\n",
      "caramel   ->>>>   TASTING NOTES\n",
      "nut   ->>>>   TASTING NOTES\n",
      "washed   ->>>>   PROCESSING\n",
      "4430 ft   ->>>>   ELEVATION\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text using the loaded spaCy NER model\n",
    "doc = nlp(text_cleaner(text))\n",
    "\n",
    "# Iterate through the named entities (entities) recognized by the model\n",
    "for ent in doc.ents:\n",
    "  # Print the recognized text and its corresponding label\n",
    "  print(ent.text, \"  ->>>>  \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
