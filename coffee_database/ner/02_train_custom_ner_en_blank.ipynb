{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and install any necessary packages\n",
    "import spacy\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the directory containing the attribute_cleaner module\n",
    "sys.path.append(os.path.abspath(\"../transformer\"))\n",
    "\n",
    "# Now you can import text_cleaner\n",
    "from attribute_cleaner.general_string_cleaner import text_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    }
   ],
   "source": [
    "# reading the data in doccano jsonl output format\n",
    "with open('data/merged_data.jsonl', 'r') as f:\n",
    "    lines = list(f)\n",
    "\n",
    "training_data: list = []\n",
    "\n",
    "for line in lines:\n",
    "    row = json.loads(line)\n",
    "    if row['label']:\n",
    "        training_data.append(  [ row[\"text\"], { \"entities\": row[\"label\"] } ] )\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"elevation : 1500 masl variety : s795 processing : burundi washed fermentation : two-stage anaerobic fermentation roast : medium-light producers : shreeraksha purnesh tasting notes : red grapes , sweet lime , hazelnuts sreeraksha is one of those planters without whom most coffee businesses will fail . he understands and supports , ensuring mutual success , not only does he produce incredible coffee , but he is also a fantastic person . the baarbara estate burundi washed is one of the most pleasant coffees we tasted this year , with its complex dual fermentation ( dry and wet ) and superbly focussed sorting and meticulous picking . this coffee is\\'simply\\'a delight . the roast profile is medium-light ; here , the longer development helps caramelise the sugars and make them easily soluble in the coffee . the complex fermentation brings out the flavours of red grapes , with the sweet acidity of sweet lime and a delicious hazelnutty aftertaste , making it a crowd-pleaser . this medium-light roast does well in both filter and espresso brews , and makes some smashing milk beverages\"',\n",
       " {'entities': [[13, 22, 'ELEVATION'],\n",
       "   [33, 37, 'VARIETAL'],\n",
       "   [51, 65, 'PROCESSING'],\n",
       "   [122, 134, 'ROAST LEVEL'],\n",
       "   [147, 166, 'FARMER'],\n",
       "   [183, 193, 'TASTING NOTES'],\n",
       "   [196, 206, 'TASTING NOTES'],\n",
       "   [209, 218, 'TASTING NOTES'],\n",
       "   [219, 229, 'FARMER'],\n",
       "   [694, 706, 'ROAST LEVEL'],\n",
       "   [864, 874, 'TASTING NOTES'],\n",
       "   [902, 913, 'TASTING NOTES'],\n",
       "   [930, 940, 'TASTING NOTES'],\n",
       "   [987, 999, 'ROAST LEVEL']]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create spaCy DocBin objects from the annotated data\n",
    "def get_spacy_doc(file, data):\n",
    "  # Create a blank spaCy pipeline\n",
    "  nlp = spacy.blank('en')\n",
    "  db = DocBin()\n",
    "\n",
    "  # Iterate through the data\n",
    "  for text, annot in tqdm(data):\n",
    "    doc = nlp.make_doc(text)\n",
    "    annot = annot['entities']\n",
    "\n",
    "    ents = []\n",
    "    entity_indices = []\n",
    "\n",
    "    # Extract entities from the annotations\n",
    "    for start, end, label in annot:\n",
    "      skip_entity = False\n",
    "      for idx in range(start, end):\n",
    "        if idx in entity_indices:\n",
    "          skip_entity = True\n",
    "          break\n",
    "      if skip_entity:\n",
    "        continue\n",
    "\n",
    "      entity_indices = entity_indices + list(range(start, end))\n",
    "      try:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "      if span is None:\n",
    "        # Log errors for annotations that couldn't be processed\n",
    "        err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
    "        file.write(err_data)\n",
    "      else:\n",
    "        ents.append(span)\n",
    "\n",
    "    try:\n",
    "      doc.ents = ents\n",
    "      db.add(doc)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 1138.32it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 863.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the annotated data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(training_data, test_size=0.1)\n",
    "\n",
    "# Display the number of items in the training and testing sets\n",
    "print(len(train), len(test))\n",
    "\n",
    "# Open a file to log errors during annotation processing\n",
    "file = open('train_file_try_4.txt','w')\n",
    "\n",
    "# Create spaCy DocBin objects for training and testing data\n",
    "db = get_spacy_doc(file, train)\n",
    "db.to_disk('train_doccano_try_4.spacy')\n",
    "\n",
    "db = get_spacy_doc(file, test)\n",
    "db.to_disk('test_doccano_try_4.spacy')\n",
    "\n",
    "# Close the error log file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config_cpu_en_blank.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config_cpu_en_blank.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# configure spacy for custom NER model , using a base config file - https://spacy.io/usage/training#config\n",
    "\n",
    "# !python -m spacy init fill-config config_cpu_en_blank_base.cfg config_cpu_en_blank.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: output_try_4\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: output_try_4\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     99.88    0.00    0.00    0.00    0.00\n",
      "  0      50        299.45   2391.04    0.00    0.00    0.00    0.00\n",
      "  0     100        462.22   1068.04   42.46   49.29   37.30    0.42\n",
      "  0     150        307.02    911.28   30.56   30.32   30.81    0.31\n",
      "  1     200        880.29    865.58   49.46   49.73   49.19    0.49\n",
      "  1     250        306.06    814.23   53.92   64.18   46.49    0.54\n",
      "  1     300        186.18    952.95   52.43   52.43   52.43    0.52\n",
      "  2     350        439.59    808.87   58.09   74.58   47.57    0.58\n",
      "  2     400        325.14    735.45   59.20   58.42   60.00    0.59\n",
      "  2     450         66.66    559.03   57.29   56.25   58.38    0.57\n",
      "  3     500         24.12    566.27   57.14   53.27   61.62    0.57\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output_try_4/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config_cpu_en_blank.cfg  --output  output_try_4  --paths.train train_doccano_try_4.spacy  --paths.dev  test_doccano_try_4.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and trying the model with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy library\n",
    "import spacy\n",
    "\n",
    "# Load the trained spaCy NER model from the specified path\n",
    "nlp = spacy.load('output_try_4/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Jasmine kissed cranberry - 100 % arabica that is sourced from Chikmagalur, grown at an altitude of approximately 4,100ft to 4,500ft. It gets its unique name through a process which include a strain of yeast used to create a carbon dioxide-rich environment during fermentation in stainless-steel fermenters. A remarkable level of complexity in the beans is created by this process, laying the groundwork for a wonderful cup of coffee. Steady drying on raised beds is the next important step, which lets the flavors gradually develop and intensify. After another 30 days of continuous stirring, the coffee undergo additional drying developing scent of jasmine, complex notes of raspberries, cranberries, and sparkling malic acidity with a lingering floral aftertaste \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jasmine   ->>>>   TASTING NOTES\n",
      "cranberry   ->>>>   TASTING NOTES\n",
      "arabica   ->>>>   COFFEE TYPE\n",
      "chikmagalur   ->>>>   LOCATION\n",
      "500ft   ->>>>   ELEVATION\n",
      "jasmine   ->>>>   TASTING NOTES\n",
      "raspberries   ->>>>   TASTING NOTES\n",
      "cranberries   ->>>>   TASTING NOTES\n",
      "floral   ->>>>   TASTING NOTES\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text using the loaded spaCy NER model\n",
    "doc = nlp(text_cleaner(text))\n",
    "\n",
    "# Iterate through the named entities (entities) recognized by the model\n",
    "for ent in doc.ents:\n",
    "  # Print the recognized text and its corresponding label\n",
    "  print(ent.text, \"  ->>>>  \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"\n",
    "Single origin Indian Coffee From Moganad Estate, Tamil Nadu which is a 100% Arabiaca coffee with \n",
    "an exquisite blend of balanced sweetness and brightness. A Medium Dark Roast coffee with \n",
    "flavor notes of Cocoa, Caramel and Nut which can also be enjoyed in a French press, moka pot, aeropress & espresso .\n",
    " It is washed processed, grown at an altitude of around 4430 ft\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moganad   ->>>>   ESTATE\n",
      "tamil nadu   ->>>>   LOCATION\n",
      "arabiaca   ->>>>   COFFEE TYPE\n",
      "dark   ->>>>   ROAST LEVEL\n",
      "cocoa   ->>>>   TASTING NOTES\n",
      "caramel   ->>>>   TASTING NOTES\n",
      "nut   ->>>>   TASTING NOTES\n",
      "washed   ->>>>   PROCESSING\n",
      "4430 ft   ->>>>   ELEVATION\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text using the loaded spaCy NER model\n",
    "doc = nlp(text_cleaner(text))\n",
    "\n",
    "# Iterate through the named entities (entities) recognized by the model\n",
    "for ent in doc.ents:\n",
    "  # Print the recognized text and its corresponding label\n",
    "  print(ent.text, \"  ->>>>  \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
