{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and install any necessary packages\n",
    "import spacy\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the directory containing the attribute_cleaner module\n",
    "sys.path.append(os.path.abspath(\"../transformer\"))\n",
    "\n",
    "# Now you can import text_cleaner\n",
    "from attribute_cleaner.general_string_cleaner import text_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    }
   ],
   "source": [
    "# reading the data in doccano jsonl output format\n",
    "with open('data/merged_data.jsonl', 'r') as f:\n",
    "    lines = list(f)\n",
    "\n",
    "training_data: list = []\n",
    "\n",
    "for line in lines:\n",
    "    row = json.loads(line)\n",
    "    if row['label']:\n",
    "        training_data.append(  [ row[\"text\"], { \"entities\": row[\"label\"] } ] )\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"our og best seller is back and this time , with an update ! this year\\'s iteration of mind=blown is a yeast inoculated natural . the folks at mooleh manay wanted consistency across the board with their naturals and honeys this year . using yeast , was the best way to ensure this . for this year\\'s lot , ripe cherries were picked and put in a floatation tank as usual to remove immatures . the cherries that sank were then put in airtight food grade barrels and a yeast culture was added . fermentation was carefully monitored and after 64 hours the coffee was put out to dry slowly on raised beds in a polyhouse ( the new normal thanks to unseasonal harvest rains ) . all this care has resulted in a coffee that is fruity but also clean and insanely sweet . we find the acidity very raspberry like and the sweetness reminds us of vanilla . we cannot state enough about how sweet and drinkable this year\\'s iteration of mind=blown is . the coffee is very versatile and is a delight across all brews , manual ( degassing time 3 days ) and espresso ( degassing time 14 days )\"',\n",
       " {'entities': [[86, 96, 'NAME'],\n",
       "   [102, 125, 'PROCESSING'],\n",
       "   [142, 154, 'ESTATE'],\n",
       "   [537, 545, 'FERMENTATION TIME'],\n",
       "   [732, 737, 'BODY'],\n",
       "   [784, 793, 'TASTING NOTES'],\n",
       "   [831, 838, 'TASTING NOTES'],\n",
       "   [919, 929, 'NAME']]}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create spaCy DocBin objects from the annotated data\n",
    "def get_spacy_doc(file, data):\n",
    "  # Create a blank spaCy pipeline\n",
    "  nlp = spacy.blank('en')\n",
    "  db = DocBin()\n",
    "\n",
    "  # Iterate through the data\n",
    "  for text, annot in tqdm(data):\n",
    "    doc = nlp.make_doc(text)\n",
    "    annot = annot['entities']\n",
    "\n",
    "    ents = []\n",
    "    entity_indices = []\n",
    "\n",
    "    # Extract entities from the annotations\n",
    "    for start, end, label in annot:\n",
    "      skip_entity = False\n",
    "      for idx in range(start, end):\n",
    "        if idx in entity_indices:\n",
    "          skip_entity = True\n",
    "          break\n",
    "      if skip_entity:\n",
    "        continue\n",
    "\n",
    "      entity_indices = entity_indices + list(range(start, end))\n",
    "      try:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "      if span is None:\n",
    "        # Log errors for annotations that couldn't be processed\n",
    "        err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
    "        file.write(err_data)\n",
    "      else:\n",
    "        ents.append(span)\n",
    "\n",
    "    try:\n",
    "      doc.ents = ents\n",
    "      db.add(doc)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:00<00:00, 1341.52it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 739.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the annotated data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(training_data, test_size=0.05)\n",
    "\n",
    "# Display the number of items in the training and testing sets\n",
    "print(len(train), len(test))\n",
    "\n",
    "# Open a file to log errors during annotation processing\n",
    "file = open('train_file_try_5.txt','w')\n",
    "\n",
    "# Create spaCy DocBin objects for training and testing data\n",
    "db = get_spacy_doc(file, train)\n",
    "db.to_disk('train_doccano_try_5.spacy')\n",
    "\n",
    "db = get_spacy_doc(file, test)\n",
    "db.to_disk('test_doccano_try_5.spacy')\n",
    "\n",
    "# Close the error log file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure spacy for custom NER model , using a base config file - https://spacy.io/usage/training#config\n",
    "\n",
    "# !python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output_try_5\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "/Users/snehil/.dev_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/snehil/.dev_env/lib/python3.12/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "/Users/snehil/.dev_env/lib/python3.12/site-packages/thinc/shims/pytorch.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "  0       0        1024.75   1632.33    0.00    0.00    0.00    0.00\n",
      "  0      10       11724.31  18552.70    0.00    0.00    0.00    0.00\n",
      "  0      20        7731.22  12128.73    0.00    0.00    0.00    0.00\n",
      "  1      30         652.59   1529.54    0.00    0.00    0.00    0.00\n",
      "  1      40        2413.81   2381.04    5.97   11.32    4.05    0.06\n",
      "  2      50         777.15   1634.90    0.00    0.00    0.00    0.00\n",
      "  2      60       12479.89   2033.31   27.00   51.92   18.24    0.27\n",
      "  3      70         745.67   1481.25    5.19   66.67    2.70    0.05\n",
      "  3      80       10549.52   1598.77   50.59   60.95   43.24    0.51\n",
      "  4      90        2925.37   1544.49   49.17   64.13   39.86    0.49\n",
      "  4     100        3914.17   1320.06   42.80   47.15   39.19    0.43\n",
      "  5     110        1476.27   1369.19   53.42   49.43   58.11    0.53\n",
      "  5     120       15805.44   1186.70   54.36   56.12   52.70    0.54\n",
      "  5     130        5358.11   1653.61   53.33   63.55   45.95    0.53\n",
      "  6     140         475.18    847.86   52.89   68.09   43.24    0.53\n",
      "  6     150       11767.66   1567.16   46.10   48.51   43.92    0.46\n",
      "  7     160         625.27    823.30   54.47   64.22   47.30    0.54\n",
      "  7     170       16463.58   1552.50   51.72   52.82   50.68    0.52\n",
      "  8     180        1930.84   1035.33   58.17   70.87   49.32    0.58\n",
      "  8     190       11174.54   1161.25   60.25   55.75   65.54    0.60\n",
      "  9     200        5692.06   1027.63   60.90   62.41   59.46    0.61\n",
      "  9     210        2919.37    803.85   57.33   55.35   59.46    0.57\n",
      " 10     220        5368.67   1497.56   56.16   56.94   55.41    0.56\n",
      " 10     230         312.63    586.09   61.02   61.22   60.81    0.61\n",
      " 10     240         802.02    821.12   58.54   60.43   56.76    0.59\n",
      " 11     250       22027.04   1449.21   59.40   66.95   53.38    0.59\n",
      " 11     260        1166.82    823.30   60.18   53.40   68.92    0.60\n",
      " 12     270         472.99    544.14   62.45   69.42   56.76    0.62\n",
      " 12     280        3880.55    740.91   64.43   64.00   64.86    0.64\n",
      " 13     290         723.93    571.72   61.94   69.17   56.08    0.62\n",
      " 13     300         607.29    585.18   65.43   60.23   71.62    0.65\n",
      " 14     310        1071.53    564.58   62.58   59.88   65.54    0.63\n",
      " 14     320         446.68    502.17   61.65   65.65   58.11    0.62\n",
      " 15     330         484.05    583.68   66.08   58.64   75.68    0.66\n",
      " 15     340         302.91    402.34   56.68   70.71   47.30    0.57\n",
      " 15     350         305.88    474.71   63.04   54.73   74.32    0.63\n",
      " 16     360         477.22    482.07   64.12   73.68   56.76    0.64\n",
      " 16     370        1036.00    729.82   64.42   58.99   70.95    0.64\n",
      " 17     380         596.93    420.68   66.45   63.03   70.27    0.66\n",
      " 17     390         711.55    557.51   66.91   71.54   62.84    0.67\n",
      " 18     400         351.89    470.55   66.28   58.55   76.35    0.66\n",
      " 18     410         437.41    486.47   64.34   75.45   56.08    0.64\n",
      " 19     420         319.48    503.79   67.46   60.43   76.35    0.67\n",
      " 19     430         252.10    406.11   67.51   63.31   72.30    0.68\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output_try_5/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg  --output  output_try_5  --paths.train train_doccano_try_5.spacy  --paths.dev  test_doccano_try_5.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and trying the model with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snehil/.dev_env/lib/python3.12/site-packages/spacy_transformers/layers/hf_shim.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Import the spaCy library\n",
    "import spacy\n",
    "\n",
    "# Load the trained spaCy NER model from the specified path\n",
    "nlp = spacy.load('output_try_3/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Jasmine kissed cranberry - 100 % arabica that is sourced from Chikmagalur, grown at an altitude of approximately 4,100ft to 4,500ft. It gets its unique name through a process which include a strain of yeast used to create a carbon dioxide-rich environment during fermentation in stainless-steel fermenters. A remarkable level of complexity in the beans is created by this process, laying the groundwork for a wonderful cup of coffee. Steady drying on raised beds is the next important step, which lets the flavors gradually develop and intensify. After another 30 days of continuous stirring, the coffee undergo additional drying developing scent of jasmine, complex notes of raspberries, cranberries, and sparkling malic acidity with a lingering floral aftertaste \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snehil/.dev_env/lib/python3.12/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jasmine   ->>>>   TASTING NOTES\n",
      "arabica   ->>>>   COFFEE TYPE\n",
      "chikmagalur   ->>>>   LOCATION\n",
      "4 , 100ft   ->>>>   ELEVATION\n",
      "jasmine   ->>>>   TASTING NOTES\n",
      "raspberries   ->>>>   TASTING NOTES\n",
      "cranberries   ->>>>   TASTING NOTES\n",
      "malic   ->>>>   ACIDITY\n",
      "floral   ->>>>   COFFEE_PROPERTIES\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text using the loaded spaCy NER model\n",
    "doc = nlp(text_cleaner(text))\n",
    "\n",
    "# Iterate through the named entities (entities) recognized by the model\n",
    "for ent in doc.ents:\n",
    "  # Print the recognized text and its corresponding label\n",
    "  print(ent.text, \"  ->>>>  \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"\n",
    "Single origin Indian Coffee From Moganad Estate, Tamil Nadu which is a 100% Arabiaca coffee with \n",
    "an exquisite blend of balanced sweetness and brightness. A Medium Dark Roast coffee with \n",
    "flavor notes of Cocoa, Caramel and Nut which can also be enjoyed in a French press, moka pot, aeropress & espresso .\n",
    " It is washed processed, grown at an altitude of around 4430 ft\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moganad   ->>>>   ESTATE\n",
      "tamil nadu   ->>>>   LOCATION\n",
      "arabiaca   ->>>>   COFFEE TYPE\n",
      "medium dark   ->>>>   ROAST LEVEL\n",
      "cocoa   ->>>>   TASTING NOTES\n",
      "caramel   ->>>>   TASTING NOTES\n",
      "nut   ->>>>   TASTING NOTES\n",
      "washed   ->>>>   PROCESSING\n",
      "4430 ft   ->>>>   ELEVATION\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text using the loaded spaCy NER model\n",
    "doc = nlp(text_cleaner(text))\n",
    "\n",
    "# Iterate through the named entities (entities) recognized by the model\n",
    "for ent in doc.ents:\n",
    "  # Print the recognized text and its corresponding label\n",
    "  print(ent.text, \"  ->>>>  \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
